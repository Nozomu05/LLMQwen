# ============================================
# MODEL PROVIDER CONFIGURATION
# ============================================
# Choose your LLM provider: ollama, mistral, or openai
MODEL_PROVIDER=ollama

# --------------------------------------------
# OLLAMA SETTINGS (Local models: Qwen, Mistral, Llama, etc.)
# --------------------------------------------
OLLAMA_MODEL=qwen2.5:7b-instruct
OLLAMA_BASE_URL=http://localhost:11434

# --------------------------------------------
# MISTRAL AI SETTINGS (Cloud API)
# --------------------------------------------
# Get your API key from: https://console.mistral.ai/
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest
# Other options: mistral-medium-latest, mistral-small-latest

# --------------------------------------------
# OPENAI SETTINGS (Cloud API)
# --------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
# Other options: gpt-4o, gpt-4-turbo, gpt-3.5-turbo

# ============================================
# DOCUMENT PROCESSING
# ============================================
DOCS_DIR=docs
CHROMA_DIR=storage/chroma

# Performance tuning for heavy documents
BATCH_SIZE=100          # Number of documents to process at once
CHUNK_SIZE=800          # Characters per chunk (smaller = better for longer docs)
CHUNK_OVERLAP=160       # Overlap between chunks (prevents context loss)

# ============================================
# EMBEDDING MODEL CONFIGURATION
# ============================================
# Embeddings convert text into vectors for semantic search
# MUST match between ingestion and query!
#
# EMBEDDING_PROVIDER options:
#   - ollama: Local models via Ollama (recommended for control & quality)
#   - fastembed: Lightweight CPU-friendly models (faster, less accurate)
#
# RECOMMENDED EMBEDDING MODELS (Ollama):
#   - mxbai-embed-large (1024-dim, 669MB) ✓ BEST for technical docs
#   - nomic-embed-text (768-dim, 274MB) - Good balance
#   - bge-large (1024-dim, 1.34GB) - Highest quality, slower
#
# For FastEmbed:
#   - BAAI/bge-small-en-v1.5 (quality)
#   - sentence-transformers/all-MiniLM-L6-v2 (speed)

EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=mxbai-embed-large

# ============================================
# RETRIEVAL & RERANKING PIPELINE
# ============================================
# How the RAG system finds relevant information:
#
# STEP 1: SEMANTIC SEARCH (RETRIEVAL_CHUNKS)
#   - Searches ALL documents in your database
#   - Compares your question to every chunk's embedding
#   - Returns the most similar chunks
#   - Example: Top 100 most similar chunks from 3000+ total
#
# STEP 2: RERANKING (TOP_N_RERANK)
#   - Takes chunks from Step 1
#   - Uses Flashrank model to re-score them more accurately
#   - Keeps only the BEST chunks
#   - Example: Best 8 out of 100
#   - ⚠️ RAM Usage: ~12.5 MB per chunk being reranked
#
# STEP 3: LLM RECEIVES FINAL CHUNKS
#   - LLM sees only TOP_N_RERANK chunks
#   - Generates answer based on those chunks
#
# CONFIGURATION GUIDE:
# --------------------
# RETRIEVAL_CHUNKS: Cast a wide net (searches all docs, returns top N)
#   - Recommended: 50-100 for good coverage
#   - Max by RAM: 8GB→300, 16GB→700, 32GB→1500
#   - ⚠️ Higher = more RAM used in reranking!
#
# TOP_N_RERANK: Final chunks sent to LLM
#   - qwen2.5:7b → 8-10 chunks max (model overwhelmed beyond this)
#   - qwen2.5:14b → 12-15 chunks
#   - qwen2.5:32b → 25-30 chunks
#   - ⚠️ More chunks ≠ better answers with smaller models!

RETRIEVAL_CHUNKS=50      # Wide net: retrieve 50 most similar chunks
TOP_N_RERANK=10          # Precision: keep best 10 for LLM (optimal for 7b)
USE_RERANKING=true       # Enable smart filtering (highly recommended)