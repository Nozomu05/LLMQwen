# ============================================
# MODEL PROVIDER CONFIGURATION
# ============================================
MODEL_PROVIDER=ollama

# --------------------------------------------
# OLLAMA SETTINGS (Local models: Qwen, Mistral, Llama, etc.)
# --------------------------------------------
# CURRENT: Mid-end configuration (16GB RAM, mid-end GPU/CPU)
OLLAMA_MODEL=qwen2.5:14b-instruct
OLLAMA_BASE_URL=http://localhost:11434

# ============================================
# HARDWARE-BASED MODEL RECOMMENDATIONS
# ============================================
# Choose the configuration that matches your hardware:

# --- [1] LOW-END CONFIG (8GB RAM, low-end GPU, low-end CPU) ---
# Best for: Budget laptops, older computers, testing
# OLLAMA_MODEL=qwen2.5:3b-instruct
# Alternative: qwen2.5:7b-instruct (if you can spare the RAM)
# Expected speed: 5-10 seconds/query (CPU), 1-3 seconds (GPU)
# RAM usage: ~4-6GB

# --- [2] MID-END CONFIG (16GB RAM, mid-end GPU, mid-end CPU) --- ✅ CURRENT
# Best for: Modern laptops, standard workstations
# OLLAMA_MODEL=qwen2.5:14b-instruct
# Alternative: qwen2.5:7b-instruct (faster)
# Expected speed: 8-15 seconds/query (CPU), 2-4 seconds (GPU)
# RAM usage: ~10-14GB

# --- [3] BETTER MID-END CONFIG (16GB RAM, modern GPU, modern CPU) ---
# Best for: Gaming PCs, modern workstations with RTX/RX GPUs
# OLLAMA_MODEL=qwen2.5:14b-instruct
# Alternative: qwen2.5:32b-instruct (if GPU has 12GB+ VRAM)
# Expected speed: 1-2 seconds/query (good GPU), 6-10 seconds (CPU)
# RAM usage: ~12-16GB
# Note: Modern GPU makes huge difference, can handle 32b with quantization

# --- [4] HIGH-END CONFIG (32GB+ RAM, high-end GPU, high-end CPU) ---
# Best for: Workstations, servers, RTX 4090/A6000, ThreadRipper/Xeon
# OLLAMA_MODEL=qwen2.5:32b-instruct
# Alternatives:
#   qwen2.5:72b-instruct    (64GB+ RAM)
#   qwen2.5:110b-instruct   (80-128GB RAM, ultimate quality)
# Expected speed: 0.5-1.5 seconds/query (high-end GPU), 15-30 seconds (CPU)
# RAM usage: ~24-40GB (32b), ~50-80GB (72b), ~90-120GB (110b)

# ============================================
# DOCUMENT PROCESSING
# ============================================
DOCS_DIR=docs
CHROMA_DIR=storage/chroma

# Performance tuning for heavy documents
BATCH_SIZE=500          # Number of documents to process at once (fast model = faster ingestion)

# --- CHUNKING CONFIGURATION ---
# How documents are split affects answer quality!
# Chunk size = characters per chunk | Overlap = shared chars between adjacent chunks

# CURRENT: General technical documents (good balance)
CHUNK_SIZE=800          # ~150-200 words, 2-3 paragraphs
CHUNK_OVERLAP=100       # 12.5% overlap prevents context loss at boundaries

# ADJUST BASED ON YOUR DOCUMENT TYPE:
# Dense technical specs/standards (MPEG, ISO, etc.):
#   CHUNK_SIZE=1000-1200, CHUNK_OVERLAP=150-200
#   → Preserves complete technical descriptions, tables, multi-paragraph explanations
#
# Short Q&A, FAQs, snippets:
#   CHUNK_SIZE=500-600, CHUNK_OVERLAP=75-100
#   → More precision, faster retrieval, good for focused answers
#
# Long-form articles, research papers:
#   CHUNK_SIZE=1500-2000, CHUNK_OVERLAP=300-400
#   → Preserves narrative flow, complete arguments, methodology sections
#
# Mixed document collection:
#   CHUNK_SIZE=800-1000, CHUNK_OVERLAP=120-150
#   → Balanced for various content types
#
# NOTE: Changing these requires re-ingestion: python rag/ingest.py

# ============================================
# EMBEDDING MODEL CONFIGURATION
# ============================================

# --- EMBEDDING PROVIDER SELECTION ---
# Choose: "fastembed" (faster, limited models) or "huggingface" (flexible, any model)
EMBEDDING_PROVIDER=huggingface

# CURRENT: Mid-end configuration (multilingual)
EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct

# ============================================
# HOW TO SWITCH EMBEDDING PROVIDERS
# ============================================
# 
# OPTION 1: FastEmbed (faster, optimized, limited model support)
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-m3
# 
# OPTION 2: HuggingFace (flexible, supports any model, slightly slower)
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
#
# ⚠️ After changing provider or model: run `python rag/ingest.py`

# ============================================
# HARDWARE-BASED EMBEDDING MODEL RECOMMENDATIONS
# ============================================
# Choose the configuration that matches your hardware:

# --- [1] LOW-END CONFIG (8GB RAM, low-end GPU, low-end CPU) ---
# Best for: Budget systems, fast ingestion, English-only or primary language focus
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
# Specs: 384-dim, 22M params, 5x faster than bge-m3
# Quality: Good for English, poor for other languages
# Ingestion speed: ~500-800 docs/minute
# RAM usage: ~500MB
# Multilingual: ❌ English-only optimized
# Provider: ✅ FastEmbed supported
#
# Multilingual alternative for low-end:
# EMBEDDING_PROVIDER=fastembed  # or huggingface
# EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# Specs: 384-dim, slower than all-MiniLM but supports 50+ languages
# Quality: Good for most languages
# Ingestion speed: ~300-500 docs/minute
# Provider: ✅ Both FastEmbed and HuggingFace

# --- [2] MID-END CONFIG (16GB RAM, mid-end GPU, mid-end CPU) --- 
# Best for: Multilingual document collections, good balance
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-m3
# Specs: 1024-dim, 568M params, excellent multilingual (100+ languages)
# Quality: Excellent for all languages
# Ingestion speed: ~150-250 docs/minute
# RAM usage: ~2GB
# Multilingual: ✅ Excellent (100+ languages)
# Provider: ✅ FastEmbed supported
#
# English-only alternative for mid-end (faster):
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-base-en-v1.5
# Specs: 768-dim, 2x faster than bge-m3, English-only
# Quality: Excellent for English
# Ingestion speed: ~250-400 docs/minute
# Provider: ✅ FastEmbed supported

# --- [3] BETTER MID-END CONFIG (16GB RAM, modern GPU, modern CPU) --- ✅ CURRENT
# Best for: High-quality multilingual RAG, modern systems
# Option A: Use FastEmbed (faster)
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-m3
#
# Option B: Use HuggingFace for better complex query understanding
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
# Specs: 1024-dim, 560M params, instruction-tuned for RAG
# Quality: Excellent+ (better understanding of complex queries)
# Ingestion speed: ~120-200 docs/minute
# RAM usage: ~2.5GB
# Multilingual: ✅ Excellent (100+ languages)
# Note: E5-large-instruct handles technical jargon and complex questions better

# --- [4] HIGH-END CONFIG (32GB+ RAM, high-end GPU, high-end CPU) ---
# Best for: Maximum quality, production systems, critical applications
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
# Specs: 1024-dim, 560M params, instruction-tuned, state-of-the-art RAG
# Quality: Best available for open-source multilingual embeddings
# Ingestion speed: ~100-180 docs/minute (with parallelization)
# RAM usage: ~2.5-3GB
# Multilingual: ✅ Excellent (100+ languages)
# Best for: Complex technical documents, research papers, precise retrieval
# Provider: ⚠️ HuggingFace only (not supported by FastEmbed)
#
# Alternative (if you need even larger context):
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
# English-only but highest quality for English technical documents

# ============================================
# RETRIEVAL & RERANKING CONFIGURATION
# ============================================
RETRIEVAL_CHUNKS=100  # Initial chunks to retrieve (more = better recall, recommended: 50-200)
TOP_N_RERANK=8        # Keep best N after reranking (recommended: 5-15)
USE_RERANKING=true    # Enable reranking for better relevance (highly recommended)

# CURRENT: Mid-end configuration (multilingual)
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# ============================================
# HARDWARE-BASED RERANKER MODEL RECOMMENDATIONS
# ============================================
# Choose the configuration that matches your hardware:

# --- [1] LOW-END CONFIG (8GB RAM, low-end GPU, low-end CPU) ---
# Best for: Budget systems, fast queries
# Option A: Disable reranking for maximum speed
# USE_RERANKING=false
# TOP_N_RERANK=8  # Not used when reranking disabled
# Speed: Instant (no reranking overhead)
#
# Option B: Lightweight reranker (recommended if you can spare 2GB RAM)
# RERANKER_MODEL=BAAI/bge-reranker-base
# Specs: 278M params, English-focused but works for other languages
# Quality: Good
# Query overhead: +0.3-0.5 seconds
# RAM usage: ~1-1.5GB
# Multilingual: ⚠️ English-focused, acceptable for others

# --- [2] MID-END CONFIG (16GB RAM, mid-end GPU, mid-end CPU) --- ✅ CURRENT
# Best for: Multilingual systems, good quality/speed balance
# RERANKER_MODEL=BAAI/bge-reranker-v2-m3
# Specs: 568M params, state-of-the-art multilingual reranker
# Quality: Excellent for all languages
# Query overhead: +0.5-1 second
# RAM usage: ~2-3GB
# Multilingual: ✅ Excellent (100+ languages)
# Top N: 8-10 recommended
#
# English-only alternative (slightly faster):
# RERANKER_MODEL=BAAI/bge-reranker-large
# Specs: 560M params, English-only, similar speed to v2-m3
# Quality: Excellent for English

# --- [3] BETTER MID-END CONFIG (16GB RAM, modern GPU, modern CPU) ---
# Best for: Modern systems with good CPU, multilingual quality
# RERANKER_MODEL=BAAI/bge-reranker-v2-m3
# Specs: Same as mid-end, but modern CPU handles it faster
# Quality: Excellent
# Query overhead: +0.3-0.6 seconds (faster CPU)
# RAM usage: ~2-3GB
# Top N: 10-12 recommended (can afford more reranking)
# Note: Good CPU makes reranking much faster, can increase TOP_N_RERANK

# --- [4] HIGH-END CONFIG (32GB+ RAM, high-end GPU, high-end CPU) ---
# Best for: Maximum accuracy, production systems
# RERANKER_MODEL=BAAI/bge-reranker-v2-minicpm-layerwise
# Specs: 2.4B params, state-of-the-art quality, multilingual
# Quality: Best available (4x more parameters than v2-m3)
# Query overhead: +1.5-3 seconds (3-5x slower than v2-m3)
# RAM usage: ~8-16GB (just for reranker!)
# Multilingual: ✅ Excellent (100+ languages)
# Top N: 12-15 recommended
# Best for: Critical applications where accuracy > speed
#
# Alternative (balanced high-end):
# RERANKER_MODEL=BAAI/bge-reranker-v2-m3
# Use current model but with higher TOP_N_RERANK=15
# Faster queries while maintaining excellent quality
