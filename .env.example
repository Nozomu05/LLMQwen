# ============================================
# MODEL PROVIDER CONFIGURATION
# ============================================
# Choose your LLM provider: ollama, mistral, or openai
MODEL_PROVIDER=ollama

# --------------------------------------------
# OLLAMA SETTINGS (Local models: Qwen, Mistral, Llama, etc.)
# --------------------------------------------
# Current: Excellent balance of speed and quality (14B params, requires 16GB RAM)
OLLAMA_MODEL=qwen2.5:14b-instruct
OLLAMA_BASE_URL=http://localhost:11434

# Alternative LLM models (run 'ollama pull <model>' first):
# OLLAMA_MODEL=qwen2.5:7b-instruct      # Faster: 7B params, requires 8GB RAM, 2x faster
# OLLAMA_MODEL=qwen2.5:32b-instruct     # Better: 32B params, requires 32GB RAM, higher quality
# OLLAMA_MODEL=qwen2.5:72b-instruct     # Best: 72B params, requires 64GB RAM, maximum quality

# ============================================
# DOCUMENT PROCESSING
# ============================================
DOCS_DIR=docs
CHROMA_DIR=storage/chroma

# Performance tuning for heavy documents
BATCH_SIZE=500          # Number of documents to process at once (fast model = faster ingestion)

# --- CHUNKING CONFIGURATION ---
# How documents are split affects answer quality!
# Chunk size = characters per chunk | Overlap = shared chars between adjacent chunks

# CURRENT: General technical documents (good balance)
CHUNK_SIZE=800          # ~150-200 words, 2-3 paragraphs
CHUNK_OVERLAP=100       # 12.5% overlap prevents context loss at boundaries

# ADJUST BASED ON YOUR DOCUMENT TYPE:
# Dense technical specs/standards (MPEG, ISO, etc.):
#   CHUNK_SIZE=1000-1200, CHUNK_OVERLAP=150-200
#   → Preserves complete technical descriptions, tables, multi-paragraph explanations
#
# Short Q&A, FAQs, snippets:
#   CHUNK_SIZE=500-600, CHUNK_OVERLAP=75-100
#   → More precision, faster retrieval, good for focused answers
#
# Long-form articles, research papers:
#   CHUNK_SIZE=1500-2000, CHUNK_OVERLAP=300-400
#   → Preserves narrative flow, complete arguments, methodology sections
#
# Mixed document collection:
#   CHUNK_SIZE=800-1000, CHUNK_OVERLAP=120-150
#   → Balanced for various content types
#
# NOTE: Changing these requires re-ingestion: python rag/ingest.py

# ============================================
# EMBEDDING MODEL CONFIGURATION
# ============================================
# FastEmbed provider (local, fast, no Ollama dependency)
EMBEDDING_PROVIDER=fastembed

# --- EMBEDDING MODEL OPTIONS ---
# Current: FAST model for testing/development (384-dim, 5x faster)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Production alternatives (uncomment to upgrade):
# EMBEDDING_MODEL=BAAI/bge-base-en-v1.5      # Balanced: 768-dim, 2x faster than large
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5     # Best Quality: 1024-dim, excellent retrieval
# EMBEDDING_MODEL=BAAI/bge-m3                # Multilingual: 1024-dim, 100+ languages

# ============================================
# RETRIEVAL & RERANKING CONFIGURATION
# ============================================
RETRIEVAL_CHUNKS=100  # Initial chunks to retrieve (more = better recall, recommended: 50-200)
TOP_N_RERANK=8        # Keep best N after reranking (recommended: 5-15)
USE_RERANKING=true    # Enable reranking for better relevance (highly recommended)

# --- RERANKER MODEL OPTIONS ---
# Current: Good balance of speed and quality (278M params)
RERANKER_MODEL=BAAI/bge-reranker-base

# Production alternatives (uncomment to upgrade):
# RERANKER_MODEL=BAAI/bge-reranker-large     # Higher quality: 560M params, 2x slower
# RERANKER_MODEL=BAAI/bge-reranker-v2-m3     # State-of-the-art: 568M params, multilingual
