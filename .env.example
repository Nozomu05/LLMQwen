# ============================================
# MODEL PROVIDER CONFIGURATION
# ============================================
LLM_PROVIDER=transformers

# --------------------------------------------
# TRANSFORMERS SETTINGS (HuggingFace models)
# --------------------------------------------
TRANSFORMERS_MODEL=Qwen/Qwen2.5-7B-Instruct
MAX_NEW_TOKENS=2048
TEMPERATURE=0.7
# Model will download to ~/.cache/huggingface/ (about 28GB for 14B model)
# QUANTIZATION: Reduces memory usage (important for GPUs with limited VRAM)
#   4bit - Use 4-bit quantization (~3.5GB VRAM for 14B model, recommended for GPUs with <12GB VRAM)
#   8bit - Use 8-bit quantization (~7GB VRAM for 14B model)
#   none - No quantization (~14GB VRAM for 14B model, requires high-end GPU)
QUANTIZATION=4bit
# MAX_NEW_TOKENS: Increase for longer, more complete answers. 2048 allows detailed responses.

# --------------------------------------------
# DEVICE CONFIGURATION
# --------------------------------------------
# Control which device each component uses
# Options:
#   auto  - Auto-detect GPU and use it if available (recommended for LLM)
#   cuda  - Force GPU usage (fastest, requires NVIDIA GPU with CUDA)
#   cpu   - Force CPU usage (slower but works on any computer)
#
# Components:
LLM_DEVICE=auto              # Qwen language model (main inference)
EMBEDDING_DEVICE=cuda        # Document/query embeddings (10-50x faster on GPU)
RERANKER_DEVICE=cuda         # Re-ranking model (improves result quality)
#
# Recommendations:
#   - With GPU: Use cuda/auto for all (best performance)
#   - CPU only: Set all to cpu
#   - Limited GPU memory: LLM=cpu, EMBEDDING=cuda, RERANKER=cpu

# ============================================
# HARDWARE-BASED MODEL RECOMMENDATIONS
# ============================================
# Choose the configuration that matches your hardware:

# Copy of .env for reference and quick setup; edit as needed.

# ============================================
# MODEL PROVIDER CONFIGURATION
# ============================================
LLM_PROVIDER=transformers

# --------------------------------------------
# TRANSFORMERS SETTINGS (HuggingFace models)
# --------------------------------------------
TRANSFORMERS_MODEL=Qwen/Qwen2.5-14B-Instruct
MAX_NEW_TOKENS=4096
TEMPERATURE=0
LLM_SEED=42
# Model will download to ~/.cache/huggingface/ (about 28GB for 14B model)
# QUANTIZATION: Reduces memory usage (important for GPUs with limited VRAM)
#   4bit - Use 4-bit quantization (~3.5GB VRAM for 14B model, recommended for GPUs with <12GB VRAM)
#   8bit - Use 8-bit quantization (~7GB VRAM for 14B model)
#   none - No quantization (~14GB VRAM for 14B model, requires high-end GPU)
QUANTIZATION=4bit
# MAX_NEW_TOKENS: Increase for longer, more complete answers. 4096 allows longer responses.

# --------------------------------------------
# DEVICE CONFIGURATION
# --------------------------------------------
# Control which device each component uses
# Options:
#   auto  - Auto-detect GPU and use it if available (recommended for LLM)
#   cuda  - Force GPU usage (fastest, requires NVIDIA GPU with CUDA)
#   cpu   - Force CPU usage (slower but works on any computer)
#
# Components:
LLM_DEVICE=auto              # Qwen language model (main inference)
EMBEDDING_DEVICE=cuda        # Document/query embeddings (10-50x faster on GPU)
RERANKER_DEVICE=cuda         # Re-ranking model (improves result quality)
#
# Recommendations:
#   - With GPU: Use cuda/auto for all (best performance)
#   - CPU only: Set all to cpu
#   - Limited GPU memory: LLM=cpu, EMBEDDING=cuda, RERANKER=cpu

# ============================================
# HARDWARE-BASED MODEL RECOMMENDATIONS
# ============================================
# Choose the configuration that matches your hardware:

# --- [1] LOW-END CONFIG (8GB RAM, low-end GPU, low-end CPU) ---
# Best for: Budget laptops, older computers, testing
# TRANSFORMERS_MODEL=Qwen/Qwen2.5-3B-Instruct
# Alternative: Qwen/Qwen2.5-7B-Instruct (if you can spare the RAM)
# Expected speed: 5-10 seconds/query (CPU), 1-3 seconds (GPU)
# RAM usage: ~4-6GB

# --- [2] MID-END CONFIG (16GB RAM, mid-end GPU, mid-end CPU) --- ✅ CURRENT
# Best for: Modern laptops, standard workstations
# TRANSFORMERS_MODEL=Qwen/Qwen2.5-14B-Instruct
# Alternative: Qwen/Qwen2.5-7B-Instruct (faster)
# Expected speed: 8-15 seconds/query (CPU), 2-4 seconds (GPU)
# RAM usage: ~10-14GB

# --- [3] BETTER MID-END CONFIG (16GB RAM, modern GPU, modern CPU) ---
# Best for: Gaming PCs, modern workstations with RTX/RX GPUs
# TRANSFORMERS_MODEL=Qwen/Qwen2.5-14B-Instruct
# Alternative: Qwen/Qwen2.5-32B-Instruct (if GPU has 12GB+ VRAM)
# Expected speed: 1-2 seconds/query (good GPU), 6-10 seconds (CPU)
# RAM usage: ~12-16GB
# Note: Modern GPU makes huge difference, can handle 32B models

# --- [4] HIGH-END CONFIG (32GB+ RAM, high-end GPU, high-end CPU) ---
# Best for: Workstations, servers, RTX 4090/A6000, ThreadRipper/Xeon
# TRANSFORMERS_MODEL=Qwen/Qwen2.5-32B-Instruct
# Alternatives:
#   Qwen/Qwen2.5-72B-Instruct    (64GB+ RAM)
# Expected speed: 0.5-1.5 seconds/query (high-end GPU), 15-30 seconds (CPU)
# RAM usage: ~24-40GB (32B), ~50-80GB (72B)

# ============================================
# DOCUMENT PROCESSING
# ============================================
DOCS_DIR=docs
CHROMA_DIR=storage/chroma

# Performance tuning for heavy documents
BATCH_SIZE=500          # Number of documents to process at once (fast model = faster ingestion)

# --- CHUNKING CONFIGURATION ---
# How documents are split affects answer quality!
# Chunk size = characters per chunk | Overlap = shared chars between adjacent chunks

# CURRENT: General technical documents (good balance)
CHUNK_SIZE=800          # ~150-200 words, 2-3 paragraphs
CHUNK_OVERLAP=100       # 12.5% overlap prevents context loss at boundaries

# ADJUST BASED ON YOUR DOCUMENT TYPE:
# Dense technical specs/standards (MPEG, ISO, etc.):
#   CHUNK_SIZE=1000-1200, CHUNK_OVERLAP=150-200
#   → Preserves complete technical descriptions, tables, multi-paragraph explanations
#
# Short Q&A, FAQs, snippets:
#   CHUNK_SIZE=500-600, CHUNK_OVERLAP=75-100
#   → More precision, faster retrieval, good for focused answers
#
# Long-form articles, research papers:
#   CHUNK_SIZE=1500-2000, CHUNK_OVERLAP=300-400
#   → Preserves narrative flow, complete arguments, methodology sections
#
# Mixed document collection:
#   CHUNK_SIZE=800-1000, CHUNK_OVERLAP=120-150
#   → Balanced for various content types
#
# NOTE: Changing these requires re-ingestion: python rag/ingest.py

# ============================================
# EMBEDDING MODEL CONFIGURATION
# ============================================

# --- EMBEDDING PROVIDER SELECTION ---
# Choose: "fastembed" (faster, limited models) or "huggingface" (flexible, any model)
EMBEDDING_PROVIDER=huggingface

# CURRENT: Mid-end configuration (multilingual)
EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct

# ============================================
# HOW TO SWITCH EMBEDDING PROVIDERS
# ============================================
# 
# OPTION 1: FastEmbed (faster, optimized, limited model support)
*** End Patch
# --- CHUNKING CONFIGURATION ---
# How documents are split affects answer quality!
# Chunk size = characters per chunk | Overlap = shared chars between adjacent chunks

# CURRENT: General technical documents (good balance)
CHUNK_SIZE=800          # ~150-200 words, 2-3 paragraphs
CHUNK_OVERLAP=100       # 12.5% overlap prevents context loss at boundaries

# ADJUST BASED ON YOUR DOCUMENT TYPE:
# Dense technical specs/standards (MPEG, ISO, etc.):
#   CHUNK_SIZE=1000-1200, CHUNK_OVERLAP=150-200
#   → Preserves complete technical descriptions, tables, multi-paragraph explanations
#
# Short Q&A, FAQs, snippets:
#   CHUNK_SIZE=500-600, CHUNK_OVERLAP=75-100
#   → More precision, faster retrieval, good for focused answers
#
# Long-form articles, research papers:
#   CHUNK_SIZE=1500-2000, CHUNK_OVERLAP=300-400
#   → Preserves narrative flow, complete arguments, methodology sections
#
# Mixed document collection:
#   CHUNK_SIZE=800-1000, CHUNK_OVERLAP=120-150
#   → Balanced for various content types
#
# NOTE: Changing these requires re-ingestion: python rag/ingest.py

# ============================================
# EMBEDDING MODEL CONFIGURATION
# ============================================

# --- EMBEDDING PROVIDER SELECTION ---
# Choose: "fastembed" (faster, limited models) or "huggingface" (flexible, any model)
EMBEDDING_PROVIDER=huggingface

# CURRENT: Mid-end configuration (multilingual)
EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct

# ============================================
# HOW TO SWITCH EMBEDDING PROVIDERS
# ============================================
# 
# OPTION 1: FastEmbed (faster, optimized, limited model support)
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-m3
# 
# OPTION 2: HuggingFace (flexible, supports any model, slightly slower)
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
#
# ⚠️ After changing provider or model: run `python rag/ingest.py`

# ============================================
# HARDWARE-BASED EMBEDDING MODEL RECOMMENDATIONS
# ============================================
# Choose the configuration that matches your hardware:

# --- [1] LOW-END CONFIG (8GB RAM, low-end GPU, low-end CPU) ---
# Best for: Budget systems, fast ingestion, English-only or primary language focus
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
# Specs: 384-dim, 22M params, 5x faster than bge-m3
# Quality: Good for English, poor for other languages
# Ingestion speed: ~500-800 docs/minute
# RAM usage: ~500MB
# Multilingual: ❌ English-only optimized
# Provider: ✅ FastEmbed supported
#
# Multilingual alternative for low-end:
# EMBEDDING_PROVIDER=fastembed  # or huggingface
# EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# Specs: 384-dim, slower than all-MiniLM but supports 50+ languages
# Quality: Good for most languages
# Ingestion speed: ~300-500 docs/minute
# Provider: ✅ Both FastEmbed and HuggingFace
#
# --- [2] MID-END CONFIG (16GB RAM, mid-end GPU, mid-end CPU) --- 
# Best for: Multilingual document collections, good balance
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-m3
# Specs: 1024-dim, 568M params, excellent multilingual (100+ languages)
# Quality: Excellent for all languages
# Ingestion speed: ~150-250 docs/minute
# RAM usage: ~2GB
# Multilingual: ✅ Excellent (100+ languages)
# Provider: ✅ FastEmbed supported
#
# English-only alternative for mid-end (faster):
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-base-en-v1.5
# Specs: 768-dim, 2x faster than bge-m3, English-only
# Quality: Excellent for English
# Ingestion speed: ~250-400 docs/minute
# Provider: ✅ FastEmbed supported
#
# --- [3] BETTER MID-END CONFIG (16GB RAM, modern GPU, modern CPU) --- ✅ CURRENT
# Best for: High-quality multilingual RAG, modern systems
# Option A: Use FastEmbed (faster)
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-m3
#
# Option B: Use HuggingFace for better complex query understanding
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
# Specs: 1024-dim, 560M params, instruction-tuned for RAG
# Quality: Excellent+ (better understanding of complex queries)
# Ingestion speed: ~120-200 docs/minute
# RAM usage: ~2.5GB
# Multilingual: ✅ Excellent (100+ languages)
# Note: E5-large-instruct handles technical jargon and complex questions better
#
# --- [4] HIGH-END CONFIG (32GB+ RAM, high-end GPU, high-end CPU) ---
# Best for: Maximum quality, production systems, critical applications
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
# Specs: 1024-dim, 560M params, instruction-tuned, state-of-the-art RAG
# Quality: Best available for open-source multilingual embeddings
# Ingestion speed: ~100-180 docs/minute (with parallelization)
# RAM usage: ~2.5-3GB
# Multilingual: ✅ Excellent (100+ languages)
# Best for: Complex technical documents, research papers, precise retrieval
# Provider: ⚠️ HuggingFace only (not supported by FastEmbed)
#
# Alternative (if you need even larger context):
# EMBEDDING_PROVIDER=fastembed
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
# English-only but highest quality for English technical documents
#
# ============================================
# RETRIEVAL & RERANKING CONFIGURATION
# ============================================
RETRIEVAL_CHUNKS=100  # Initial chunks to retrieve (more = better recall, recommended: 50-200)
TOP_N_RERANK=8        # Keep best N after reranking (recommended: 5-15)
USE_RERANKING=true    # Enable reranking for better relevance (highly recommended)

# CURRENT: Mid-end configuration (multilingual)
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# ============================================
# HARDWARE-BASED RERANKER MODEL RECOMMENDATIONS
# ============================================
# Choose the configuration that matches your hardware:

# --- [1] LOW-END CONFIG (8GB RAM, low-end GPU, low-end CPU) ---
# Best for: Budget systems, fast queries
# Option A: Disable reranking for maximum speed
# USE_RERANKING=false
# TOP_N_RERANK=8  # Not used when reranking disabled
# Speed: Instant (no reranking overhead)
#
# Option B: Lightweight reranker (recommended if you can spare 2GB RAM)
# RERANKER_MODEL=BAAI/bge-reranker-base
# Specs: 278M params, English-focused but works for other languages
# Quality: Good
# Query overhead: +0.3-0.5 seconds
# RAM usage: ~1-1.5GB
# Multilingual: ⚠️ English-focused, acceptable for others
#
# --- [2] MID-END CONFIG (16GB RAM, mid-end GPU, mid-end CPU) --- ✅ CURRENT
# Best for: Multilingual systems, good quality/speed balance
# RERANKER_MODEL=BAAI/bge-reranker-v2-m3
# Specs: 568M params, state-of-the-art multilingual reranker
# Quality: Excellent for all languages
# Query overhead: +0.5-1 second
# RAM usage: ~2-3GB
# Multilingual: ✅ Excellent (100+ languages)
# Top N: 8-10 recommended
#
# English-only alternative (slightly faster):
# RERANKER_MODEL=BAAI/bge-reranker-large
# Specs: 560M params, English-only, similar speed to v2-m3
# Quality: Excellent for English
#
```
