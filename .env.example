# ============================================
# MODEL PROVIDER CONFIGURATION
# ============================================
# Choose your LLM provider: ollama, mistral, or openai
MODEL_PROVIDER=ollama

# --------------------------------------------
# OLLAMA SETTINGS (Local models: Qwen, Mistral, Llama, etc.)
# --------------------------------------------
OLLAMA_MODEL=qwen2.5:7b-instruct
OLLAMA_BASE_URL=http://localhost:11434

# --------------------------------------------
# MISTRAL AI SETTINGS (Cloud API)
# --------------------------------------------
# Get your API key from: https://console.mistral.ai/
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest
# Other options: mistral-medium-latest, mistral-small-latest

# --------------------------------------------
# OPENAI SETTINGS (Cloud API)
# --------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
# Other options: gpt-4o, gpt-4-turbo, gpt-3.5-turbo

# ============================================
# DOCUMENT PROCESSING
# ============================================ Document ingestion settings
DOCS_DIR=docs
CHROMA_DIR=storage/chroma

# Performance tuning for heavy documents
BATCH_SIZE=100          # Number of documents to process at once
CHUNK_SIZE=1500         # Characters per chunk (larger = more context)
CHUNK_OVERLAP=300       # Overlap between chunks (prevents context loss)

# Retrieval & accuracy settings
RETRIEVAL_CHUNKS=12     # Initial chunks to retrieve (more = better recall)
TOP_N_RERANK=6          # Keep best N after reranking (improves precision)
USE_RERANKING=true      # Enable reranking for better relevance